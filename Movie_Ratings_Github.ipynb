{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICTING USER RATINGS FOR UNSEEN MOVIES USING KERAS AND STOCHASTIC GRADIENT DESCENT\n",
    "\n",
 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "#import graphviz, pydot\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "import math\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating\n",
       "0       1     1193       5\n",
       "1       1      661       3\n",
       "2       1      914       3\n",
       "3       1     3408       4\n",
       "4       1     2355       5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory =r\"ratings1.csv\"\n",
    "\n",
    "df = pd.read_csv(directory)#, names = [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], sep = '::')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5809451634060117"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(labels = ['Rating'], axis = 1)\n",
    "Y = df.drop(labels = ['UserID','MovieID'], axis = 1)\n",
    "\n",
    "# # Train Validation and Test Split\n",
    "Train_X, Validate_X, Train_Y, Validate_Y = train_test_split(X,Y, train_size=0.50)\n",
    "Validate_X, Test_X, Validate_Y, Test_Y = train_test_split(Validate_X,Validate_Y, train_size = 0.50)\n",
    "\n",
    "\n",
    "average_ratings = np.mean(Train_Y)\n",
    "float(average_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=np.array(Train_X)\n",
    "X_val=np.array(Validate_X)\n",
    "Y_train=np.array(Train_Y)\n",
    "Y_val=np.array(Validate_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=np.array(Test_X)\n",
    "Y_test=np.array(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "We will follow the exposition on https://datajobs.com/data-science-repo/\n",
    "\n",
    "A successful approach to the prediction of the ranking matrix $r_{i,u}$ is based\n",
    "on a matrix decomposition model, where our ranking predictions are\n",
    "\n",
    "$\\hat{r}_{i,u} = \\mu+b_u+b_i+p^T_u q_i$\n",
    "\n",
    "The model parameters $\\theta = (\\mu, b_u, b_i, p_u, q_i)$ are defined as\n",
    "Mean rating $\\mu$ which is the average rating of all users over all movies in our training\n",
    "set $\\tau$\n",
    "\n",
    "$$\\mu = \\frac{1}{N_\\tau}\\sum_{(i,u)\\epsilon\\tau} r_{u,i}$$\n",
    "\n",
    "**User Bias** $b_u$ is a per user bias that will be higher for users that give high\n",
    "average ratings to all movies.\n",
    "\n",
    "**Item Bias** $b_i$ is a per item bias that will be higher for the more popular (higher\n",
    "ranked) movies.\n",
    "\n",
    "**User Embedding** $p_u  \\epsilon  R^F$ is a per user F dimensional vector that maps user\n",
    "$\\mu$ into some kind of abstract taste space.\n",
    "\n",
    "**Item Embedding** $q_i \\epsilon R^F$ is a per item F dimensional vector that maps item\n",
    "i into the taste space.\n",
    "\n",
    "The interaction dot product $p^T_u q_i$ drives users and items pointing in nearly\n",
    "parallel directions in the latent space $R^F$ towards higher ratings.\n",
    "\n",
    "The dimension F of the latent space is unknown a priory and is a model\n",
    "hyper-parameter that must be chosen by cross validation.\n",
    "\n",
    "In the special case F = 0, where there is no interaction term $p^T_u q_i$, the\n",
    "relative ranking of items are the same for all users\n",
    "\n",
    "$r_{u,i}$ $- r_{u,i'}$ = $b_i - b_{i'}$\n",
    "\n",
    "and we say that items are ranked by **popularity**.\n",
    "\n",
    "\n",
    "A recommender system that wishes to provide personalized rankings instead of just suggesting the same items to all users needs an interaction term\n",
    "$p^T_u q_i$ because the preferences of each user for particular kinds of items is encoded\n",
    "exclusively on the interaction term $p^T_u q_i$.\n",
    "\n",
    "As neither the biases, not the embeddings are know a priory, to train the\n",
    "model we need to learn the following\n",
    "\n",
    "\u000f $b_u$: U coefficients\n",
    "\n",
    "\u000f $b_i$: I coefficients\n",
    "\n",
    "\u000f $p_u$: U x F coefficients\n",
    "\n",
    "\u000f $p_i$: I x F coefficients.\n",
    "\n",
    "Provided that the embedding dimension F is small, U x (F + 1) + I x (F + 1)\n",
    "will be much smaller than the U x I entries on the ranking matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "To fully specify the model we must initialize model parameters. Initialization\n",
    "should not matter for final results provided gradient descent has converged, but\n",
    "it may significantly impact the rate of convergence.\n",
    "The following the parameters provide good convergence in practice\n",
    "\n",
    "$b^0_u \\sim \\mathcal{N}(0, 10^{-4})$\n",
    "\n",
    "$b^0_i \\sim \\mathcal{N}(0, 10^{-4})$\n",
    "\n",
    "$p^0_{u,f} \\sim \\mathcal{N}\\big(0,\\frac{1}{max(1,\\sqrt{F})}\\big)$ \n",
    "\n",
    "$q^0_{i,f} \\sim \\mathcal{N}\\big(0,\\frac{1}{max(1,\\sqrt{F})}\\big)$\n",
    "\n",
    "where the $\\sqrt{F}$ factor guarantees that the expected value of $p^T_u q_i$ is of $O(1)$, \n",
    "independently of F, for F = 0, 1, ... F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500104 samples, validate on 250052 samples\n",
      "Epoch 1/1000\n",
      "500104/500104 [==============================] - 96s 193us/step - loss: 0.9875 - val_loss: 0.9016\n",
      "Epoch 2/1000\n",
      "500104/500104 [==============================] - 96s 192us/step - loss: 0.8691 - val_loss: 0.8619\n",
      "Epoch 3/1000\n",
      "500104/500104 [==============================] - 95s 189us/step - loss: 0.8434 - val_loss: 0.8576\n",
      "Epoch 4/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8320 - val_loss: 0.8435\n",
      "Epoch 5/1000\n",
      "500104/500104 [==============================] - 95s 189us/step - loss: 0.8254 - val_loss: 0.8388\n",
      "Epoch 6/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8214 - val_loss: 0.8402\n",
      "Epoch 7/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8186 - val_loss: 0.8352\n",
      "Epoch 8/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8168 - val_loss: 0.8358\n",
      "Epoch 9/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8150 - val_loss: 0.8386\n",
      "Epoch 10/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8141 - val_loss: 0.8341\n",
      "Epoch 11/1000\n",
      "500104/500104 [==============================] - 92s 185us/step - loss: 0.8129 - val_loss: 0.8332\n",
      "Epoch 12/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8122 - val_loss: 0.8354\n",
      "Epoch 13/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8116 - val_loss: 0.8352\n",
      "Epoch 14/1000\n",
      "500104/500104 [==============================] - 94s 187us/step - loss: 0.8113 - val_loss: 0.8342\n",
      "Epoch 15/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8106 - val_loss: 0.8326\n",
      "Epoch 16/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8102 - val_loss: 0.8546\n",
      "Epoch 17/1000\n",
      "500104/500104 [==============================] - 90s 180us/step - loss: 0.8101 - val_loss: 0.8341\n",
      "Epoch 18/1000\n",
      "500104/500104 [==============================] - 91s 182us/step - loss: 0.8096 - val_loss: 0.8436\n",
      "Epoch 19/1000\n",
      "500104/500104 [==============================] - 91s 183us/step - loss: 0.8093 - val_loss: 0.8342\n",
      "Epoch 20/1000\n",
      "500104/500104 [==============================] - 92s 183us/step - loss: 0.8091 - val_loss: 0.8355\n",
      "Epoch 21/1000\n",
      "500104/500104 [==============================] - 88s 176us/step - loss: 0.8088 - val_loss: 0.8328\n",
      "Epoch 22/1000\n",
      "500104/500104 [==============================] - 89s 178us/step - loss: 0.8088 - val_loss: 0.8326\n",
      "Epoch 23/1000\n",
      "500104/500104 [==============================] - 87s 175us/step - loss: 0.8085 - val_loss: 0.8327\n",
      "Epoch 24/1000\n",
      " 71968/500104 [===>..........................] - ETA: 1:10 - loss: 0.8071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8084 - val_loss: 0.8328\n",
      "Epoch 26/1000\n",
      "500104/500104 [==============================] - 89s 179us/step - loss: 0.8080 - val_loss: 0.8328\n",
      "Epoch 27/1000\n",
      " 38176/500104 [=>............................] - ETA: 1:14 - loss: 0.8028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8078 - val_loss: 0.8329\n",
      "Epoch 29/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8076 - val_loss: 0.8328\n",
      "Epoch 30/1000\n",
      "162240/500104 [========>.....................] - ETA: 52s - loss: 0.8043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8074 - val_loss: 0.8332\n",
      "Epoch 32/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8064 - val_loss: 0.8332\n",
      "Epoch 43/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8062 - val_loss: 0.8331\n",
      "Epoch 44/1000\n",
      "500104/500104 [==============================] - 84s 167us/step - loss: 0.8062 - val_loss: 0.8331\n",
      "Epoch 45/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8062 - val_loss: 0.8332\n",
      "Epoch 46/1000\n",
      "500104/500104 [==============================] - 84s 167us/step - loss: 0.8061 - val_loss: 0.8332\n",
      "Epoch 47/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8061 - val_loss: 0.8333\n",
      "Epoch 48/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8059 - val_loss: 0.8347\n",
      "Epoch 49/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8058 - val_loss: 0.8359\n",
      "Epoch 50/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8057 - val_loss: 0.8379\n",
      "Epoch 51/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8057 - val_loss: 0.8334\n",
      "Epoch 52/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8056 - val_loss: 0.8332\n",
      "Epoch 53/1000\n",
      "500104/500104 [==============================] - 83s 167us/step - loss: 0.8055 - val_loss: 0.8333\n",
      "Epoch 54/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8055 - val_loss: 0.8356\n",
      "Epoch 55/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8055 - val_loss: 0.8334\n",
      "Epoch 56/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8053 - val_loss: 0.8332\n",
      "Epoch 57/1000\n",
      "500104/500104 [==============================] - 84s 167us/step - loss: 0.8053 - val_loss: 0.8332\n",
      "Epoch 58/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8052 - val_loss: 0.8336\n",
      "Epoch 59/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8051 - val_loss: 0.8346\n",
      "Epoch 60/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8052 - val_loss: 0.8356\n",
      "Epoch 61/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8051 - val_loss: 0.8341\n",
      "Epoch 62/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8051 - val_loss: 0.8349\n",
      "Epoch 63/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8051 - val_loss: 0.8355\n",
      "Epoch 64/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8050 - val_loss: 0.8340\n",
      "Epoch 65/1000\n",
      "500104/500104 [==============================] - 87s 174us/step - loss: 0.8050 - val_loss: 0.8359\n",
      "Epoch 66/1000\n",
      "500104/500104 [==============================] - 87s 174us/step - loss: 0.8048 - val_loss: 0.8335\n",
      "Epoch 67/1000\n",
      "500104/500104 [==============================] - 86s 173us/step - loss: 0.8049 - val_loss: 0.8344\n",
      "Epoch 68/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8047 - val_loss: 0.8333\n",
      "Epoch 69/1000\n",
      "500104/500104 [==============================] - 88s 175us/step - loss: 0.8048 - val_loss: 0.8356\n",
      "Epoch 70/1000\n",
      "500104/500104 [==============================] - 87s 173us/step - loss: 0.8047 - val_loss: 0.8334\n",
      "Epoch 71/1000\n",
      "500104/500104 [==============================] - 86s 171us/step - loss: 0.8046 - val_loss: 0.8335\n",
      "Epoch 72/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8046 - val_loss: 0.8341\n",
      "Epoch 73/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8046 - val_loss: 0.8338\n",
      "Epoch 74/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8046 - val_loss: 0.8335\n",
      "Epoch 75/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8045 - val_loss: 0.8350\n",
      "Epoch 76/1000\n",
      "500104/500104 [==============================] - 86s 171us/step - loss: 0.8044 - val_loss: 0.8363\n",
      "Epoch 77/1000\n",
      "500104/500104 [==============================] - 85s 171us/step - loss: 0.8044 - val_loss: 0.8352\n",
      "Epoch 78/1000\n",
      "500104/500104 [==============================] - 85s 171us/step - loss: 0.8044 - val_loss: 0.8366\n",
      "Epoch 79/1000\n",
      "500104/500104 [==============================] - 86s 173us/step - loss: 0.8044 - val_loss: 0.8353\n",
      "Epoch 80/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8043 - val_loss: 0.8356\n",
      "Epoch 81/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8042 - val_loss: 0.8335\n",
      "Epoch 82/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8042 - val_loss: 0.8334\n",
      "Epoch 83/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8042 - val_loss: 0.8335\n",
      "Epoch 84/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8042 - val_loss: 0.8335\n",
      "Epoch 85/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8041 - val_loss: 0.8335\n",
      "Epoch 86/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8040 - val_loss: 0.8342\n",
      "Epoch 87/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8042 - val_loss: 0.8335\n",
      "Epoch 88/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8040 - val_loss: 0.8335\n",
      "Epoch 89/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8039 - val_loss: 0.8335\n",
      "Epoch 90/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8039 - val_loss: 0.8340\n",
      "Epoch 91/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8039 - val_loss: 0.8336\n",
      "Epoch 92/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8040 - val_loss: 0.8333\n",
      "Epoch 93/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8039 - val_loss: 0.8369\n",
      "Epoch 94/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8038 - val_loss: 0.8339\n",
      "Epoch 95/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8038 - val_loss: 0.8336\n",
      "Epoch 96/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8037 - val_loss: 0.8352\n",
      "Epoch 97/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8038 - val_loss: 0.8372\n",
      "Epoch 98/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8037 - val_loss: 0.8335\n",
      "Epoch 99/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8037 - val_loss: 0.8353\n",
      "Epoch 100/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8036 - val_loss: 0.8345\n",
      "Epoch 101/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8036 - val_loss: 0.8334\n",
      "Epoch 102/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8035 - val_loss: 0.8335\n",
      "Epoch 103/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8035 - val_loss: 0.8352\n",
      "Epoch 104/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8035 - val_loss: 0.8334\n",
      "Epoch 105/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8035 - val_loss: 0.8341\n",
      "Epoch 106/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8036 - val_loss: 0.8337\n",
      "Epoch 107/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8035 - val_loss: 0.8367\n",
      "Epoch 108/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8034 - val_loss: 0.8334\n",
      "Epoch 109/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8033 - val_loss: 0.8350\n",
      "Epoch 110/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8033 - val_loss: 0.8335\n",
      "Epoch 111/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8034 - val_loss: 0.8339\n",
      "Epoch 112/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8033 - val_loss: 0.8336\n",
      "Epoch 113/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8033 - val_loss: 0.8334\n",
      "Epoch 114/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8033 - val_loss: 0.8336\n",
      "Epoch 115/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8033 - val_loss: 0.8338\n",
      "Epoch 116/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8032 - val_loss: 0.8336\n",
      "Epoch 117/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8032 - val_loss: 0.8343\n",
      "Epoch 118/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8033 - val_loss: 0.8344\n",
      "Epoch 119/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8031 - val_loss: 0.8336\n",
      "Epoch 120/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8031 - val_loss: 0.8337\n",
      "Epoch 121/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8031 - val_loss: 0.8339\n",
      "Epoch 122/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8031 - val_loss: 0.8351\n",
      "Epoch 123/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8031 - val_loss: 0.8338\n",
      "Epoch 124/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8030 - val_loss: 0.8337\n",
      "Epoch 125/1000\n",
      "500104/500104 [==============================] - 83s 167us/step - loss: 0.8030 - val_loss: 0.8338\n",
      "Epoch 126/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8031 - val_loss: 0.8351\n",
      "Epoch 127/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8031 - val_loss: 0.8335\n",
      "Epoch 128/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8029 - val_loss: 0.8341\n",
      "Epoch 129/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8030 - val_loss: 0.8335\n",
      "Epoch 130/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8030 - val_loss: 0.8336\n",
      "Epoch 131/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8029 - val_loss: 0.8336\n",
      "Epoch 132/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8029 - val_loss: 0.8334\n",
      "Epoch 133/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8029 - val_loss: 0.8340\n",
      "Epoch 134/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8029 - val_loss: 0.8364\n",
      "Epoch 135/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8029 - val_loss: 0.8341\n",
      "Epoch 136/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8028 - val_loss: 0.8335\n",
      "Epoch 137/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8028 - val_loss: 0.8349\n",
      "Epoch 138/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8028 - val_loss: 0.8337\n",
      "Epoch 139/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8027 - val_loss: 0.8345\n",
      "Epoch 140/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8027 - val_loss: 0.8337\n",
      "Epoch 141/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8028 - val_loss: 0.8336\n",
      "Epoch 142/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8027 - val_loss: 0.8335\n",
      "Epoch 143/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8026 - val_loss: 0.8342\n",
      "Epoch 144/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8026 - val_loss: 0.8338\n",
      "Epoch 145/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8026 - val_loss: 0.8336\n",
      "Epoch 146/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8026 - val_loss: 0.8339\n",
      "Epoch 147/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8026 - val_loss: 0.8363\n",
      "Epoch 148/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8026 - val_loss: 0.8363\n",
      "Epoch 149/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8026 - val_loss: 0.8336\n",
      "Epoch 150/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8026 - val_loss: 0.8341\n",
      "Epoch 151/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8026 - val_loss: 0.8336\n",
      "Epoch 152/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8024 - val_loss: 0.8335\n",
      "Epoch 153/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8025 - val_loss: 0.8337\n",
      "Epoch 154/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8025 - val_loss: 0.8336\n",
      "Epoch 155/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8025 - val_loss: 0.8336\n",
      "Epoch 156/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8025 - val_loss: 0.8337\n",
      "Epoch 157/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8025 - val_loss: 0.8338\n",
      "Epoch 158/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8023 - val_loss: 0.8336\n",
      "Epoch 159/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8024 - val_loss: 0.8336\n",
      "Epoch 160/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8024 - val_loss: 0.8335\n",
      "Epoch 161/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8024 - val_loss: 0.8343\n",
      "Epoch 162/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8024 - val_loss: 0.8338\n",
      "Epoch 163/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8023 - val_loss: 0.8351\n",
      "Epoch 164/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8024 - val_loss: 0.8336\n",
      "Epoch 165/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8023 - val_loss: 0.8339\n",
      "Epoch 166/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8024 - val_loss: 0.8337\n",
      "Epoch 167/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8022 - val_loss: 0.8334\n",
      "Epoch 168/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8023 - val_loss: 0.8335\n",
      "Epoch 169/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8023 - val_loss: 0.8355\n",
      "Epoch 170/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8022 - val_loss: 0.8338\n",
      "Epoch 171/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8022 - val_loss: 0.8341\n",
      "Epoch 172/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8022 - val_loss: 0.8341\n",
      "Epoch 173/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8022 - val_loss: 0.8347\n",
      "Epoch 174/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8021 - val_loss: 0.8338\n",
      "Epoch 175/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8022 - val_loss: 0.8337\n",
      "Epoch 176/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8022 - val_loss: 0.8342\n",
      "Epoch 177/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8022 - val_loss: 0.8338\n",
      "Epoch 178/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8022 - val_loss: 0.8340\n",
      "Epoch 179/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8021 - val_loss: 0.8345\n",
      "Epoch 180/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8021 - val_loss: 0.8361\n",
      "Epoch 181/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8021 - val_loss: 0.8339\n",
      "Epoch 182/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8021 - val_loss: 0.8337\n",
      "Epoch 183/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8021 - val_loss: 0.8336\n",
      "Epoch 184/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8021 - val_loss: 0.8336\n",
      "Epoch 185/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8021 - val_loss: 0.8341\n",
      "Epoch 186/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8021 - val_loss: 0.8335\n",
      "Epoch 187/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8020 - val_loss: 0.8336\n",
      "Epoch 188/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8021 - val_loss: 0.8342\n",
      "Epoch 189/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8020 - val_loss: 0.8340\n",
      "Epoch 190/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8020 - val_loss: 0.8340\n",
      "Epoch 191/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8019 - val_loss: 0.8337\n",
      "Epoch 192/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8020 - val_loss: 0.8342\n",
      "Epoch 193/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8020 - val_loss: 0.8336\n",
      "Epoch 194/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8020 - val_loss: 0.8339\n",
      "Epoch 195/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8019 - val_loss: 0.8339\n",
      "Epoch 196/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8020 - val_loss: 0.8336\n",
      "Epoch 197/1000\n",
      "500104/500104 [==============================] - 84s 167us/step - loss: 0.8019 - val_loss: 0.8349\n",
      "Epoch 198/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8019 - val_loss: 0.8364\n",
      "Epoch 199/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8019 - val_loss: 0.8335\n",
      "Epoch 200/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8019 - val_loss: 0.8382\n",
      "Epoch 201/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8017 - val_loss: 0.8338\n",
      "Epoch 202/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8019 - val_loss: 0.8336\n",
      "Epoch 203/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8019 - val_loss: 0.8338\n",
      "Epoch 204/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8018 - val_loss: 0.8343\n",
      "Epoch 205/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8019 - val_loss: 0.8336\n",
      "Epoch 206/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8019 - val_loss: 0.8339\n",
      "Epoch 207/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8018 - val_loss: 0.8350\n",
      "Epoch 208/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8018 - val_loss: 0.8339\n",
      "Epoch 209/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8018 - val_loss: 0.8347\n",
      "Epoch 210/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8018 - val_loss: 0.8336\n",
      "Epoch 211/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8018 - val_loss: 0.8342\n",
      "Epoch 212/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8018 - val_loss: 0.8351\n",
      "Epoch 213/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8018 - val_loss: 0.8337\n",
      "Epoch 214/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8017 - val_loss: 0.8347\n",
      "Epoch 215/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8017 - val_loss: 0.8338\n",
      "Epoch 216/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8017 - val_loss: 0.8339\n",
      "Epoch 217/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8017 - val_loss: 0.8345\n",
      "Epoch 218/1000\n",
      "500104/500104 [==============================] - 83s 165us/step - loss: 0.8017 - val_loss: 0.8336\n",
      "Epoch 219/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8017 - val_loss: 0.8336\n",
      "Epoch 220/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8018 - val_loss: 0.8358\n",
      "Epoch 221/1000\n",
      "500104/500104 [==============================] - 82s 165us/step - loss: 0.8017 - val_loss: 0.8354\n",
      "Epoch 222/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8017 - val_loss: 0.8336\n",
      "Epoch 223/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 224/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 225/1000\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.8015 - val_loss: 0.8339\n",
      "Epoch 226/1000\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.8016 - val_loss: 0.8338\n",
      "Epoch 227/1000\n",
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 228/1000\n",
      "500104/500104 [==============================] - 81s 163us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 229/1000\n",
      "500104/500104 [==============================] - 84s 167us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 230/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8016 - val_loss: 0.8337\n",
      "Epoch 231/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8016 - val_loss: 0.8336\n",
      "Epoch 232/1000\n",
      "500104/500104 [==============================] - 83s 166us/step - loss: 0.8015 - val_loss: 0.8336\n",
      "Epoch 233/1000\n",
      "500104/500104 [==============================] - 82s 163us/step - loss: 0.8015 - val_loss: 0.8339\n",
      "Epoch 234/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8015 - val_loss: 0.8339\n",
      "Epoch 235/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8015 - val_loss: 0.8339\n",
      "Epoch 236/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8016 - val_loss: 0.8337\n",
      "Epoch 237/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8015 - val_loss: 0.8337\n",
      "Epoch 238/1000\n",
      "500104/500104 [==============================] - 79s 159us/step - loss: 0.8015 - val_loss: 0.8336\n",
      "Epoch 239/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8015 - val_loss: 0.8336\n",
      "Epoch 240/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8015 - val_loss: 0.8354\n",
      "Epoch 241/1000\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.8015 - val_loss: 0.8337\n",
      "Epoch 242/1000\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.8015 - val_loss: 0.8339\n",
      "Epoch 243/1000\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.8015 - val_loss: 0.8337\n",
      "Epoch 244/1000\n",
      "500104/500104 [==============================] - 82s 164us/step - loss: 0.8015 - val_loss: 0.8353\n",
      "Epoch 245/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8015 - val_loss: 0.8337\n",
      "Epoch 246/1000\n",
      "500104/500104 [==============================] - 87s 174us/step - loss: 0.8014 - val_loss: 0.8336\n",
      "Epoch 247/1000\n",
      "500104/500104 [==============================] - 90s 181us/step - loss: 0.8014 - val_loss: 0.8338\n",
      "Epoch 248/1000\n",
      "500104/500104 [==============================] - 91s 181us/step - loss: 0.8014 - val_loss: 0.8340\n",
      "Epoch 249/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8014 - val_loss: 0.8339\n",
      "Epoch 250/1000\n",
      "500104/500104 [==============================] - 94s 189us/step - loss: 0.8014 - val_loss: 0.8337\n",
      "Epoch 251/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8014 - val_loss: 0.8341\n",
      "Epoch 252/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8014 - val_loss: 0.8345\n",
      "Epoch 253/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8014 - val_loss: 0.8337\n",
      "Epoch 254/1000\n",
      "500104/500104 [==============================] - 95s 189us/step - loss: 0.8014 - val_loss: 0.8336\n",
      "Epoch 255/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8014 - val_loss: 0.8337\n",
      "Epoch 256/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8013 - val_loss: 0.8338\n",
      "Epoch 257/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8013 - val_loss: 0.8343\n",
      "Epoch 258/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8014 - val_loss: 0.8337\n",
      "Epoch 259/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8013 - val_loss: 0.8342\n",
      "Epoch 260/1000\n",
      "500104/500104 [==============================] - 91s 181us/step - loss: 0.8013 - val_loss: 0.8337\n",
      "Epoch 261/1000\n",
      "500104/500104 [==============================] - 91s 181us/step - loss: 0.8013 - val_loss: 0.8340\n",
      "Epoch 262/1000\n",
      "500104/500104 [==============================] - 91s 181us/step - loss: 0.8013 - val_loss: 0.8338\n",
      "Epoch 263/1000\n",
      "500104/500104 [==============================] - 90s 181us/step - loss: 0.8013 - val_loss: 0.8337\n",
      "Epoch 264/1000\n",
      "500104/500104 [==============================] - 91s 183us/step - loss: 0.8013 - val_loss: 0.8339\n",
      "Epoch 265/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8013 - val_loss: 0.8347\n",
      "Epoch 266/1000\n",
      "500104/500104 [==============================] - 94s 189us/step - loss: 0.8013 - val_loss: 0.8349\n",
      "Epoch 267/1000\n",
      "500104/500104 [==============================] - 95s 190us/step - loss: 0.8013 - val_loss: 0.8337\n",
      "Epoch 268/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8013 - val_loss: 0.8341\n",
      "Epoch 269/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8014 - val_loss: 0.8336\n",
      "Epoch 270/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8013 - val_loss: 0.8337\n",
      "Epoch 271/1000\n",
      "500104/500104 [==============================] - 94s 187us/step - loss: 0.8013 - val_loss: 0.8336\n",
      "Epoch 272/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8012 - val_loss: 0.8338\n",
      "Epoch 273/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8012 - val_loss: 0.8342\n",
      "Epoch 274/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8012 - val_loss: 0.8345\n",
      "Epoch 275/1000\n",
      "500104/500104 [==============================] - 92s 185us/step - loss: 0.8013 - val_loss: 0.8339\n",
      "Epoch 276/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8012 - val_loss: 0.8339\n",
      "Epoch 277/1000\n",
      "500104/500104 [==============================] - 95s 190us/step - loss: 0.8012 - val_loss: 0.8339\n",
      "Epoch 278/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8012 - val_loss: 0.8347\n",
      "Epoch 279/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8012 - val_loss: 0.8341\n",
      "Epoch 280/1000\n",
      "500104/500104 [==============================] - 94s 187us/step - loss: 0.8012 - val_loss: 0.8340\n",
      "Epoch 281/1000\n",
      "500104/500104 [==============================] - 95s 190us/step - loss: 0.8012 - val_loss: 0.8340\n",
      "Epoch 282/1000\n",
      "500104/500104 [==============================] - 97s 194us/step - loss: 0.8012 - val_loss: 0.8337\n",
      "Epoch 283/1000\n",
      "500104/500104 [==============================] - 97s 194us/step - loss: 0.8012 - val_loss: 0.8336\n",
      "Epoch 284/1000\n",
      "500104/500104 [==============================] - 106s 213us/step - loss: 0.8011 - val_loss: 0.8338\n",
      "Epoch 285/1000\n",
      "500104/500104 [==============================] - 105s 210us/step - loss: 0.8012 - val_loss: 0.8344\n",
      "Epoch 286/1000\n",
      "500104/500104 [==============================] - 100s 200us/step - loss: 0.8011 - val_loss: 0.8356\n",
      "Epoch 287/1000\n",
      "500104/500104 [==============================] - 102s 204us/step - loss: 0.8011 - val_loss: 0.8354\n",
      "Epoch 288/1000\n",
      "500104/500104 [==============================] - 98s 196us/step - loss: 0.8012 - val_loss: 0.8346\n",
      "Epoch 289/1000\n",
      "500104/500104 [==============================] - 103s 207us/step - loss: 0.8012 - val_loss: 0.8336\n",
      "Epoch 290/1000\n",
      "500104/500104 [==============================] - 100s 200us/step - loss: 0.8011 - val_loss: 0.8345\n",
      "Epoch 291/1000\n",
      "500104/500104 [==============================] - 99s 197us/step - loss: 0.8011 - val_loss: 0.8337\n",
      "Epoch 292/1000\n",
      "500104/500104 [==============================] - 104s 207us/step - loss: 0.8010 - val_loss: 0.8337\n",
      "Epoch 293/1000\n",
      "500104/500104 [==============================] - 98s 196us/step - loss: 0.8011 - val_loss: 0.8336\n",
      "Epoch 294/1000\n",
      "500104/500104 [==============================] - 99s 198us/step - loss: 0.8011 - val_loss: 0.8338\n",
      "Epoch 295/1000\n",
      "500104/500104 [==============================] - 104s 209us/step - loss: 0.8011 - val_loss: 0.8349\n",
      "Epoch 296/1000\n",
      "500104/500104 [==============================] - 105s 210us/step - loss: 0.8011 - val_loss: 0.8347\n",
      "Epoch 297/1000\n",
      "500104/500104 [==============================] - 105s 209us/step - loss: 0.8011 - val_loss: 0.8337\n",
      "Epoch 298/1000\n",
      "500104/500104 [==============================] - 102s 205us/step - loss: 0.8011 - val_loss: 0.8337\n",
      "Epoch 299/1000\n",
      "500104/500104 [==============================] - 103s 206us/step - loss: 0.8011 - val_loss: 0.8339\n",
      "Epoch 300/1000\n",
      "500104/500104 [==============================] - 98s 195us/step - loss: 0.8011 - val_loss: 0.8339\n",
      "Epoch 301/1000\n",
      "500104/500104 [==============================] - 97s 194us/step - loss: 0.8011 - val_loss: 0.8337\n",
      "Epoch 302/1000\n",
      "500104/500104 [==============================] - 100s 199us/step - loss: 0.8011 - val_loss: 0.8338\n",
      "Epoch 303/1000\n",
      "500104/500104 [==============================] - 94s 189us/step - loss: 0.8011 - val_loss: 0.8340\n",
      "Epoch 304/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8010 - val_loss: 0.8343\n",
      "Epoch 305/1000\n",
      "500104/500104 [==============================] - 99s 199us/step - loss: 0.8011 - val_loss: 0.8336\n",
      "Epoch 306/1000\n",
      "500104/500104 [==============================] - 97s 194us/step - loss: 0.8010 - val_loss: 0.8341\n",
      "Epoch 307/1000\n",
      "500104/500104 [==============================] - 97s 194us/step - loss: 0.8010 - val_loss: 0.8337\n",
      "Epoch 308/1000\n",
      "500104/500104 [==============================] - 97s 195us/step - loss: 0.8010 - val_loss: 0.8340\n",
      "Epoch 309/1000\n",
      "500104/500104 [==============================] - 96s 192us/step - loss: 0.8010 - val_loss: 0.8338\n",
      "Epoch 310/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8010 - val_loss: 0.8338\n",
      "Epoch 311/1000\n",
      "500104/500104 [==============================] - 97s 193us/step - loss: 0.8010 - val_loss: 0.8338\n",
      "Epoch 312/1000\n",
      "500104/500104 [==============================] - 97s 195us/step - loss: 0.8010 - val_loss: 0.8339\n",
      "Epoch 313/1000\n",
      "500104/500104 [==============================] - 96s 193us/step - loss: 0.8010 - val_loss: 0.8348\n",
      "Epoch 314/1000\n",
      "500104/500104 [==============================] - 97s 193us/step - loss: 0.8010 - val_loss: 0.8338\n",
      "Epoch 315/1000\n",
      "500104/500104 [==============================] - 98s 195us/step - loss: 0.8010 - val_loss: 0.8340\n",
      "Epoch 316/1000\n",
      "500104/500104 [==============================] - 94s 187us/step - loss: 0.8010 - val_loss: 0.8337\n",
      "Epoch 317/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8010 - val_loss: 0.8364\n",
      "Epoch 318/1000\n",
      "500104/500104 [==============================] - 93s 187us/step - loss: 0.8009 - val_loss: 0.8339\n",
      "Epoch 319/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8010 - val_loss: 0.8336\n",
      "Epoch 320/1000\n",
      "500104/500104 [==============================] - 91s 182us/step - loss: 0.8010 - val_loss: 0.8359\n",
      "Epoch 321/1000\n",
      "500104/500104 [==============================] - 91s 181us/step - loss: 0.8010 - val_loss: 0.8336\n",
      "Epoch 322/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8009 - val_loss: 0.8337\n",
      "Epoch 323/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8009 - val_loss: 0.8338\n",
      "Epoch 324/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 325/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8010 - val_loss: 0.8339\n",
      "Epoch 326/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 327/1000\n",
      "500104/500104 [==============================] - 93s 186us/step - loss: 0.8009 - val_loss: 0.8347\n",
      "Epoch 328/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8009 - val_loss: 0.8344\n",
      "Epoch 329/1000\n",
      "500104/500104 [==============================] - 91s 182us/step - loss: 0.8009 - val_loss: 0.8339\n",
      "Epoch 330/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 331/1000\n",
      "500104/500104 [==============================] - 91s 183us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 332/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8009 - val_loss: 0.8345\n",
      "Epoch 333/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8009 - val_loss: 0.8342\n",
      "Epoch 334/1000\n",
      "500104/500104 [==============================] - 93s 185us/step - loss: 0.8009 - val_loss: 0.8337\n",
      "Epoch 335/1000\n",
      "500104/500104 [==============================] - 91s 183us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 336/1000\n",
      "500104/500104 [==============================] - 90s 181us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 337/1000\n",
      "500104/500104 [==============================] - 89s 178us/step - loss: 0.8009 - val_loss: 0.8342\n",
      "Epoch 338/1000\n",
      "500104/500104 [==============================] - 90s 179us/step - loss: 0.8009 - val_loss: 0.8352\n",
      "Epoch 339/1000\n",
      "500104/500104 [==============================] - 92s 184us/step - loss: 0.8008 - val_loss: 0.8341\n",
      "Epoch 340/1000\n",
      "500104/500104 [==============================] - 91s 182us/step - loss: 0.8009 - val_loss: 0.8336\n",
      "Epoch 341/1000\n",
      "500104/500104 [==============================] - 89s 178us/step - loss: 0.8008 - val_loss: 0.8338\n",
      "Epoch 342/1000\n",
      "500104/500104 [==============================] - 88s 175us/step - loss: 0.8008 - val_loss: 0.8338\n",
      "Epoch 343/1000\n",
      "500104/500104 [==============================] - 86s 173us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 344/1000\n",
      "500104/500104 [==============================] - 88s 176us/step - loss: 0.8009 - val_loss: 0.8349\n",
      "Epoch 345/1000\n",
      "500104/500104 [==============================] - 94s 189us/step - loss: 0.8008 - val_loss: 0.8340\n",
      "Epoch 346/1000\n",
      "500104/500104 [==============================] - 96s 191us/step - loss: 0.8008 - val_loss: 0.8339\n",
      "Epoch 347/1000\n",
      "500104/500104 [==============================] - 96s 191us/step - loss: 0.8008 - val_loss: 0.8339\n",
      "Epoch 348/1000\n",
      "500104/500104 [==============================] - 96s 191us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 349/1000\n",
      "500104/500104 [==============================] - 89s 178us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 350/1000\n",
      "500104/500104 [==============================] - 85s 171us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 351/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8008 - val_loss: 0.8341\n",
      "Epoch 352/1000\n",
      "500104/500104 [==============================] - 86s 172us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 353/1000\n",
      "500104/500104 [==============================] - 85s 171us/step - loss: 0.8009 - val_loss: 0.8338\n",
      "Epoch 354/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8008 - val_loss: 0.8339\n",
      "Epoch 355/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 356/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8008 - val_loss: 0.8338\n",
      "Epoch 359/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 360/1000\n",
      "500104/500104 [==============================] - 86s 171us/step - loss: 0.8008 - val_loss: 0.8358\n",
      "Epoch 361/1000\n",
      "500104/500104 [==============================] - 84s 169us/step - loss: 0.8008 - val_loss: 0.8336\n",
      "Epoch 362/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8339\n",
      "Epoch 363/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8341\n",
      "Epoch 364/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 365/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8008 - val_loss: 0.8337\n",
      "Epoch 366/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 367/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8008 - val_loss: 0.8336\n",
      "Epoch 368/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8339\n",
      "Epoch 369/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8341\n",
      "Epoch 370/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8339\n",
      "Epoch 371/1000\n",
      "500104/500104 [==============================] - 85s 171us/step - loss: 0.8007 - val_loss: 0.8338\n",
      "Epoch 372/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 373/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8007 - val_loss: 0.8336\n",
      "Epoch 374/1000\n",
      "500104/500104 [==============================] - 86s 171us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 375/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8349\n",
      "Epoch 376/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8338\n",
      "Epoch 377/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8007 - val_loss: 0.8349\n",
      "Epoch 378/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8007 - val_loss: 0.8340\n",
      "Epoch 379/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8007 - val_loss: 0.8339\n",
      "Epoch 380/1000\n",
      "500104/500104 [==============================] - 85s 170us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 381/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 382/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 383/1000\n",
      "500104/500104 [==============================] - 83s 167us/step - loss: 0.8007 - val_loss: 0.8336\n",
      "Epoch 384/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 385/1000\n",
      "500104/500104 [==============================] - 85s 169us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 386/1000\n",
      "500104/500104 [==============================] - 84s 168us/step - loss: 0.8007 - val_loss: 0.8338\n",
      "Epoch 387/1000\n",
      "500104/500104 [==============================] - 91s 183us/step - loss: 0.8007 - val_loss: 0.8345\n",
      "Epoch 388/1000\n",
      "500104/500104 [==============================] - 113s 225us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 389/1000\n",
      "500104/500104 [==============================] - 116s 231us/step - loss: 0.8006 - val_loss: 0.8353\n",
      "Epoch 390/1000\n",
      "500104/500104 [==============================] - 115s 230us/step - loss: 0.8006 - val_loss: 0.8339\n",
      "Epoch 391/1000\n",
      "500104/500104 [==============================] - 118s 235us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 392/1000\n",
      "500104/500104 [==============================] - 117s 233us/step - loss: 0.8006 - val_loss: 0.8340\n",
      "Epoch 393/1000\n",
      "500104/500104 [==============================] - 116s 232us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 394/1000\n",
      "500104/500104 [==============================] - 116s 233us/step - loss: 0.8007 - val_loss: 0.8337\n",
      "Epoch 395/1000\n",
      "500104/500104 [==============================] - 117s 233us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 396/1000\n",
      "500104/500104 [==============================] - 116s 232us/step - loss: 0.8006 - val_loss: 0.8336\n",
      "Epoch 397/1000\n",
      "500104/500104 [==============================] - 118s 236us/step - loss: 0.8007 - val_loss: 0.8338\n",
      "Epoch 398/1000\n",
      "500104/500104 [==============================] - 117s 233us/step - loss: 0.8006 - val_loss: 0.8345\n",
      "Epoch 399/1000\n",
      "500104/500104 [==============================] - 116s 231us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 400/1000\n",
      "500104/500104 [==============================] - 115s 230us/step - loss: 0.8006 - val_loss: 0.8342\n",
      "Epoch 401/1000\n",
      "500104/500104 [==============================] - 115s 231us/step - loss: 0.8006 - val_loss: 0.8336\n",
      "Epoch 402/1000\n",
      "500104/500104 [==============================] - 116s 233us/step - loss: 0.8006 - val_loss: 0.8338\n",
      "Epoch 403/1000\n",
      "500104/500104 [==============================] - 115s 230us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 404/1000\n",
      "500104/500104 [==============================] - 115s 231us/step - loss: 0.8006 - val_loss: 0.8338\n",
      "Epoch 405/1000\n",
      "500104/500104 [==============================] - 117s 233us/step - loss: 0.8006 - val_loss: 0.8348\n",
      "Epoch 406/1000\n",
      "500104/500104 [==============================] - 116s 233us/step - loss: 0.8006 - val_loss: 0.8345\n",
      "Epoch 407/1000\n",
      "500104/500104 [==============================] - 116s 231us/step - loss: 0.8006 - val_loss: 0.8338\n",
      "Epoch 408/1000\n",
      "500104/500104 [==============================] - 115s 230us/step - loss: 0.8005 - val_loss: 0.8336\n",
      "Epoch 409/1000\n",
      "500104/500104 [==============================] - 114s 229us/step - loss: 0.8006 - val_loss: 0.8339\n",
      "Epoch 410/1000\n",
      "500104/500104 [==============================] - 115s 231us/step - loss: 0.8006 - val_loss: 0.8340\n",
      "Epoch 411/1000\n",
      "500104/500104 [==============================] - 114s 228us/step - loss: 0.8006 - val_loss: 0.8344\n",
      "Epoch 412/1000\n",
      "500104/500104 [==============================] - 115s 229us/step - loss: 0.8006 - val_loss: 0.8336\n",
      "Epoch 413/1000\n",
      "500104/500104 [==============================] - 116s 232us/step - loss: 0.8006 - val_loss: 0.8339\n",
      "Epoch 414/1000\n",
      "500104/500104 [==============================] - 113s 225us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 415/1000\n",
      "500104/500104 [==============================] - 111s 222us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 416/1000\n",
      "500104/500104 [==============================] - 114s 228us/step - loss: 0.8006 - val_loss: 0.8339\n",
      "Epoch 417/1000\n",
      "500104/500104 [==============================] - 113s 225us/step - loss: 0.8005 - val_loss: 0.8342\n",
      "Epoch 418/1000\n",
      "500104/500104 [==============================] - 112s 223us/step - loss: 0.8006 - val_loss: 0.8340\n",
      "Epoch 419/1000\n",
      "500104/500104 [==============================] - 112s 224us/step - loss: 0.8006 - val_loss: 0.8337\n",
      "Epoch 420/1000\n",
      "500104/500104 [==============================] - 112s 224us/step - loss: 0.8005 - val_loss: 0.8343\n",
      "Epoch 421/1000\n",
      "500104/500104 [==============================] - 113s 227us/step - loss: 0.8005 - val_loss: 0.8338\n",
      "Epoch 422/1000\n",
      "500104/500104 [==============================] - 112s 225us/step - loss: 0.8005 - val_loss: 0.8339\n",
      "Epoch 423/1000\n",
      "500104/500104 [==============================] - 113s 225us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 424/1000\n",
      "500104/500104 [==============================] - 113s 226us/step - loss: 0.8005 - val_loss: 0.8354\n",
      "Epoch 425/1000\n",
      "500104/500104 [==============================] - 111s 222us/step - loss: 0.8005 - val_loss: 0.8336\n",
      "Epoch 426/1000\n",
      "500104/500104 [==============================] - 112s 223us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 427/1000\n",
      "500104/500104 [==============================] - 112s 224us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 428/1000\n",
      "500104/500104 [==============================] - 110s 221us/step - loss: 0.8005 - val_loss: 0.8345\n",
      "Epoch 429/1000\n",
      "452032/500104 [==========================>...] - ETA: 10s - loss: 0.8000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 112s 223us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 431/1000\n",
      "500104/500104 [==============================] - 112s 224us/step - loss: 0.8005 - val_loss: 0.8338\n",
      "Epoch 432/1000\n",
      "232448/500104 [============>.................] - ETA: 56s - loss: 0.8016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 110s 219us/step - loss: 0.8005 - val_loss: 0.8338\n",
      "Epoch 434/1000\n",
      "500104/500104 [==============================] - 110s 221us/step - loss: 0.8005 - val_loss: 0.8338\n",
      "Epoch 435/1000\n",
      "187232/500104 [==========>...................] - ETA: 1:03 - loss: 0.7996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 109s 217us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 437/1000\n",
      "474976/500104 [===========================>..] - ETA: 5s - loss: 0.8008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 111s 221us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 439/1000\n",
      "500104/500104 [==============================] - 109s 218us/step - loss: 0.8005 - val_loss: 0.8345\n",
      "Epoch 440/1000\n",
      "273344/500104 [===============>..............] - ETA: 46s - loss: 0.7980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 110s 220us/step - loss: 0.8004 - val_loss: 0.8338\n",
      "Epoch 442/1000\n",
      "500104/500104 [==============================] - 107s 214us/step - loss: 0.8005 - val_loss: 0.8344\n",
      "Epoch 443/1000\n",
      " 50176/500104 [==>...........................] - ETA: 1:32 - loss: 0.7896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 108s 216us/step - loss: 0.8005 - val_loss: 0.8339\n",
      "Epoch 445/1000\n",
      "327712/500104 [==================>...........] - ETA: 35s - loss: 0.8004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 108s 215us/step - loss: 0.8005 - val_loss: 0.8338\n",
      "Epoch 447/1000\n",
      "500104/500104 [==============================] - 109s 219us/step - loss: 0.8005 - val_loss: 0.8337\n",
      "Epoch 448/1000\n",
      " 84576/500104 [====>.........................] - ETA: 1:25 - loss: 0.8077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 108s 216us/step - loss: 0.8004 - val_loss: 0.8345\n",
      "Epoch 450/1000\n",
      "500104/500104 [==============================] - 110s 220us/step - loss: 0.8000 - val_loss: 0.8338\n",
      "Epoch 659/1000\n",
      "500104/500104 [==============================] - 106s 212us/step - loss: 0.8000 - val_loss: 0.8339\n",
      "Epoch 660/1000\n",
      "500104/500104 [==============================] - 103s 206us/step - loss: 0.8000 - val_loss: 0.8340\n",
      "Epoch 661/1000\n",
      "500104/500104 [==============================] - 104s 208us/step - loss: 0.7999 - val_loss: 0.8339\n",
      "Epoch 662/1000\n",
      "500104/500104 [==============================] - 107s 214us/step - loss: 0.8000 - val_loss: 0.8338\n",
      "Epoch 663/1000\n",
      "500104/500104 [==============================] - 106s 211us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 664/1000\n",
      "500104/500104 [==============================] - 108s 217us/step - loss: 0.7999 - val_loss: 0.8344\n",
      "Epoch 665/1000\n",
      "500104/500104 [==============================] - 107s 213us/step - loss: 0.8000 - val_loss: 0.8339\n",
      "Epoch 666/1000\n",
      "500104/500104 [==============================] - 107s 213us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 667/1000\n",
      "500104/500104 [==============================] - 105s 211us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 668/1000\n",
      "500104/500104 [==============================] - 103s 206us/step - loss: 0.7999 - val_loss: 0.8337\n",
      "Epoch 669/1000\n",
      "500104/500104 [==============================] - 102s 205us/step - loss: 0.8000 - val_loss: 0.8340\n",
      "Epoch 670/1000\n",
      "500104/500104 [==============================] - 111s 221us/step - loss: 0.8000 - val_loss: 0.8347\n",
      "Epoch 671/1000\n",
      "500104/500104 [==============================] - 111s 222us/step - loss: 0.8000 - val_loss: 0.8338\n",
      "Epoch 672/1000\n",
      "500104/500104 [==============================] - 107s 215us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 673/1000\n",
      "500104/500104 [==============================] - 110s 221us/step - loss: 0.8000 - val_loss: 0.8338\n",
      "Epoch 674/1000\n",
      "500104/500104 [==============================] - 111s 223us/step - loss: 0.8000 - val_loss: 0.8339\n",
      "Epoch 675/1000\n",
      "500104/500104 [==============================] - 107s 215us/step - loss: 0.7999 - val_loss: 0.8337\n",
      "Epoch 676/1000\n",
      "500104/500104 [==============================] - 104s 208us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 677/1000\n",
      "500104/500104 [==============================] - 95s 191us/step - loss: 0.8000 - val_loss: 0.8338\n",
      "Epoch 678/1000\n",
      "500104/500104 [==============================] - 94s 188us/step - loss: 0.8000 - val_loss: 0.8337\n",
      "Epoch 679/1000\n",
      "500104/500104 [==============================] - 96s 192us/step - loss: 0.8000 - val_loss: 0.8341\n",
      "Epoch 680/1000\n",
      "500104/500104 [==============================] - 95s 191us/step - loss: 0.8000 - val_loss: 0.8339\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "users (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movies (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mean (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_bias (Embedding)           (None, 1, 1)         500104      users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_bias (Embedding)          (None, 1, 1)         500104      movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mean_embeddings (Embedding)     (None, 1, 1)         500104      mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "r_hat (Add)                     (None, 1, 1)         0           user_bias[0][0]                  \n",
      "                                                                 movie_bias[0][0]                 \n",
      "                                                                 mean_embeddings[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "output (Flatten)                (None, 1)            0           r_hat[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,500,312\n",
      "Trainable params: 1,500,312\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Layer Definitions and Network Flow\n",
    "U=Train_X['UserID'].shape[0]\n",
    "M=Train_X['MovieID'].shape[0]\n",
    "mu = np.zeros(M)\n",
    "mu[:]= average_ratings   # Array of average ratings for each user.  For Training Layer\n",
    "average_ratings_shape=mu.shape[0]   #Shape of the average ratings array\n",
    "\n",
    "\n",
    "mu_validation = np.zeros(Validate_X['UserID'].shape[0])   # To be passed with validation data as the shape of this array should match with validation set\n",
    "mu_validation[:] = average_ratings\n",
    "\n",
    "# INPUT LAYERS\n",
    "users = keras.layers.Input(shape=(1,),name=\"users\")\n",
    "movies = keras.layers.Input(shape=(1,),name=\"movies\")\n",
    "avg_ratings = keras.layers.Input(shape=(1,),name=\"mean\") \n",
    "\n",
    "# EMBEDDINGS for Users, Movies, and Avg Rating\n",
    "bu= keras.layers.Embedding(U,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"user_bias\"\n",
    "                              )(users)\n",
    "bm= keras.layers.Embedding(M,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"movie_bias\"\n",
    "                              )(movies)\n",
    "u= keras.layers.Embedding(average_ratings_shape,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.Constant(value=float(average_ratings)),\n",
    "                               name=\"mean_embeddings\"\n",
    "                              )(avg_ratings)\n",
    "\n",
    "# Since this is a popularity model, we have F=0\n",
    "F=0\n",
    "penalty=1e-5\n",
    "\n",
    "# User and Movie Bias\n",
    "pu= keras.layers.Embedding(U,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"user_embedding\")(users)\n",
    "pm= keras.layers.Embedding(M,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"movies_embedding\")(movies)\n",
    "\n",
    "# INTERACTION DOT\n",
    "interaction=keras.layers.Dot(axes=-1,name=\"interaction\")([pu,pm])\n",
    "\n",
    "# AGGREGATE LAYER\n",
    "agg=keras.layers.Add(name=\"r_hat\")([bu,bm,u])\n",
    "\n",
    "#FLATTENED OUTPUT \n",
    "r_hat=keras.layers.Flatten(name=\"output\")(agg)\n",
    "\n",
    "# SETTING SGD AS OPTIMISER\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "cf_model1 = keras.models.Model(inputs=[users,movies,avg_ratings], outputs=r_hat)\n",
    "cf_model1.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "restore=False\n",
    "cp = keras.callbacks.EarlyStopping(monitor='loss',mode='min',patience=25)\n",
    "cp1=keras.callbacks.ModelCheckpoint('./model_new_1.hdf5', monitor='loss', save_best_only=True )\n",
    "cp2=keras.callbacks.CSVLogger('./model_new_1.csv', separator=',', append=False)\n",
    "if restore:\n",
    "    cf_model1.load_weights('./model_new_1.hdf5')\n",
    "\n",
    "    \n",
    "# DEPLOYING THE MODEL    \n",
    "model_train=cf_model1.fit([X_train[:,0],X_train[:,1],mu],\n",
    "                          Y_train,\n",
    "                          epochs=1000,verbose=1,\n",
    "                          validation_data=([X_val[:,0],X_val[:,1], mu_validation],Y_val),\n",
    "                         shuffle=True,callbacks=[cp,cp1,cp2], initial_epoch=0)\n",
    "cf_model1.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=model_train.history\n",
    "\n",
    "plt.plot(history[\"loss\"],label=\"Loss\")\n",
    "plt.plot(history[\"val_loss\"],label=\"Validation loss\")\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_val1=np.squeeze(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred=cf_model1.predict([X_val[:,0],X_val[:,1],mu_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred1=np.squeeze(Y_pred)\n",
    "b=[]\n",
    "for i in Y_pred1:\n",
    "    b.append(round(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY OF POPUlARITY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay of the popularity model:  0.4254874986002911\n"
     ]
    }
   ],
   "source": [
    "print(\"Accurcay of the popularity model: \",np.mean(Y_val1==b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL WITH F=10, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500104 samples, validate on 250052 samples\n",
      "Epoch 1/200\n",
      "500104/500104 [==============================] - 78s 156us/step - loss: 1.1074 - val_loss: 0.9936\n",
      "Epoch 2/200\n",
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.9393 - val_loss: 0.9298\n",
      "Epoch 3/200\n",
      "500104/500104 [==============================] - 77s 154us/step - loss: 0.8846 - val_loss: 0.9002\n",
      "Epoch 4/200\n",
      "500104/500104 [==============================] - 76s 152us/step - loss: 0.8550 - val_loss: 0.8908\n",
      "Epoch 5/200\n",
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.8359 - val_loss: 0.8742\n",
      "Epoch 6/200\n",
      "500104/500104 [==============================] - 76s 153us/step - loss: 0.8221 - val_loss: 0.8683\n",
      "Epoch 7/200\n",
      "500104/500104 [==============================] - 75s 151us/step - loss: 0.8115 - val_loss: 0.8659\n",
      "Epoch 8/200\n",
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.8029 - val_loss: 0.8612\n",
      "Epoch 9/200\n",
      "500104/500104 [==============================] - 76s 153us/step - loss: 0.7956 - val_loss: 0.8610\n",
      "Epoch 10/200\n",
      "500104/500104 [==============================] - 76s 152us/step - loss: 0.7890 - val_loss: 0.8580\n",
      "Epoch 11/200\n",
      "500104/500104 [==============================] - 76s 153us/step - loss: 0.7831 - val_loss: 0.8573\n",
      "Epoch 12/200\n",
      "500104/500104 [==============================] - 77s 153us/step - loss: 0.7721 - val_loss: 0.8600\n",
      "Epoch 14/200\n",
      "500104/500104 [==============================] - 77s 153us/step - loss: 0.7668 - val_loss: 0.8585\n",
      "Epoch 15/200\n",
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.7614 - val_loss: 0.8564\n",
      "Epoch 16/200\n",
      "352000/500104 [====================>.........] - ETA: 20s - loss: 0.7547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.7449 - val_loss: 0.8580\n",
      "Epoch 19/200\n",
      "500104/500104 [==============================] - 78s 156us/step - loss: 0.7393 - val_loss: 0.8546\n",
      "Epoch 20/200\n",
      "500104/500104 [==============================] - 69s 138us/step - loss: 0.7337 - val_loss: 0.8546\n",
      "Epoch 21/200\n",
      " 48500/500104 [=>............................] - ETA: 54s - loss: 0.7167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.7225 - val_loss: 0.8534\n",
      "Epoch 23/200\n",
      "500104/500104 [==============================] - 81s 161us/step - loss: 0.7169 - val_loss: 0.8528\n",
      "Epoch 24/200\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.7115 - val_loss: 0.8534\n",
      "Epoch 25/200\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.7063 - val_loss: 0.8560\n",
      "Epoch 26/200\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.7011 - val_loss: 0.8520\n",
      "Epoch 27/200\n",
      "500104/500104 [==============================] - 78s 157us/step - loss: 0.6961 - val_loss: 0.8522\n",
      "Epoch 28/200\n",
      "500104/500104 [==============================] - 81s 162us/step - loss: 0.6914 - val_loss: 0.8632\n",
      "Epoch 29/200\n",
      "500104/500104 [==============================] - 80s 159us/step - loss: 0.6867 - val_loss: 0.8569\n",
      "Epoch 30/200\n",
      "500104/500104 [==============================] - 80s 160us/step - loss: 0.6823 - val_loss: 0.8557\n",
      "Epoch 31/200\n",
      "500104/500104 [==============================] - 77s 155us/step - loss: 0.6780 - val_loss: 0.8523\n",
      "Epoch 32/200\n",
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.6738 - val_loss: 0.8553\n",
      "Epoch 33/200\n",
      "500104/500104 [==============================] - 80s 161us/step - loss: 0.6699 - val_loss: 0.8526\n",
      "Epoch 34/200\n",
      "500104/500104 [==============================] - 76s 153us/step - loss: 0.6660 - val_loss: 0.8535\n",
      "Epoch 35/200\n",
      "500104/500104 [==============================] - 79s 157us/step - loss: 0.6622 - val_loss: 0.8534\n",
      "Epoch 36/200\n",
      "500104/500104 [==============================] - 75s 151us/step - loss: 0.6587 - val_loss: 0.8580\n",
      "Epoch 37/200\n",
      "500104/500104 [==============================] - 78s 156us/step - loss: 0.6553 - val_loss: 0.8547\n",
      "Epoch 38/200\n",
      "500104/500104 [==============================] - 75s 150us/step - loss: 0.6519 - val_loss: 0.8545\n",
      "Epoch 39/200\n",
      "500104/500104 [==============================] - 75s 150us/step - loss: 0.6486 - val_loss: 0.8573\n",
      "Epoch 40/200\n",
      "500104/500104 [==============================] - 76s 152us/step - loss: 0.6456 - val_loss: 0.8549\n",
      "Epoch 41/200\n",
      "500104/500104 [==============================] - 75s 151us/step - loss: 0.6425 - val_loss: 0.8562\n",
      "Epoch 42/200\n",
      "500104/500104 [==============================] - 75s 150us/step - loss: 0.6396 - val_loss: 0.8582\n",
      "Epoch 43/200\n",
      "500104/500104 [==============================] - 75s 151us/step - loss: 0.6368 - val_loss: 0.8564\n",
      "Epoch 44/200\n",
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.6342 - val_loss: 0.8569\n",
      "Epoch 45/200\n",
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.6314 - val_loss: 0.8567\n",
      "Epoch 46/200\n",
      "500104/500104 [==============================] - 76s 151us/step - loss: 0.6289 - val_loss: 0.8581\n",
      "Epoch 47/200\n",
      "500104/500104 [==============================] - 78s 156us/step - loss: 0.6264 - val_loss: 0.8578\n",
      "Epoch 48/200\n",
      "500104/500104 [==============================] - 78s 155us/step - loss: 0.6239 - val_loss: 0.8583\n",
      "Epoch 49/200\n",
      "500104/500104 [==============================] - 76s 152us/step - loss: 0.6217 - val_loss: 0.8639\n",
      "Epoch 50/200\n",
      "500104/500104 [==============================] - 76s 153us/step - loss: 0.6194 - val_loss: 0.8594\n",
      "Epoch 51/200\n",
      "500104/500104 [==============================] - 79s 158us/step - loss: 0.6172 - val_loss: 0.8590\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "users (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movies (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mean (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 10)        5001040     users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movies_embedding (Embedding)    (None, 1, 10)        5001040     movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "user_bias (Embedding)           (None, 1, 1)         500104      users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_bias (Embedding)          (None, 1, 1)         500104      movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mean_embeddings (Embedding)     (None, 1, 1)         500104      mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dot)               (None, 1, 1)         0           user_embedding[0][0]             \n",
      "                                                                 movies_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "r_hat (Add)                     (None, 1, 1)         0           user_bias[0][0]                  \n",
      "                                                                 movie_bias[0][0]                 \n",
      "                                                                 mean_embeddings[0][0]            \n",
      "                                                                 interaction[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Flatten)                (None, 1)            0           r_hat[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 11,502,392\n",
      "Trainable params: 11,502,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# F=10\n",
    "# penalty = 1e-30\n",
    "# Learning rate =0.1\n",
    "# Decay Rate =1e-19\n",
    "# Batch_size=500\n",
    "U=Train_X['UserID'].shape[0]\n",
    "M=Train_X['MovieID'].shape[0]\n",
    "mu = np.zeros(M)\n",
    "mu[:]= average_ratings   # Array of average ratings for each user.  For Training Layer\n",
    "average_ratings_shape=mu.shape[0]   #Shape of the average ratings array\n",
    "\n",
    "mu_validation = np.zeros(Validate_X['UserID'].shape[0])   # To be passed with validation data as the shape of this array should match with validation set\n",
    "mu_validation[:] = average_ratings\n",
    "\n",
    "mu_test = np.zeros(Test_X['UserID'].shape[0])  \n",
    "mu_test[:] = average_ratings\n",
    "\n",
    "\n",
    "users = keras.layers.Input(shape=(1,),name=\"users\")\n",
    "movies = keras.layers.Input(shape=(1,),name=\"movies\")\n",
    "avg_ratings = keras.layers.Input(shape=(1,),name=\"mean\") \n",
    "bu= keras.layers.Embedding(U,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"user_bias\"\n",
    "                              )(users)\n",
    "bm= keras.layers.Embedding(M,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"movie_bias\"\n",
    "                              )(movies)\n",
    "u= keras.layers.Embedding(average_ratings_shape,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.Constant(value=float(average_ratings)),\n",
    "                               name=\"mean_embeddings\"\n",
    "                              )(avg_ratings)\n",
    "F=10\n",
    "penalty=1e-30\n",
    "pu= keras.layers.Embedding(U,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"user_embedding\")(users)\n",
    "pm= keras.layers.Embedding(M,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"movies_embedding\")(movies)\n",
    "interaction=keras.layers.Dot(axes=-1,name=\"interaction\")([pu,pm])\n",
    "agg=keras.layers.Add(name=\"r_hat\")([bu,bm,u,interaction])\n",
    "r_hat=keras.layers.Flatten(name=\"output\")(agg)\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-19, momentum=0.9, nesterov=True)\n",
    "\n",
    "cf_model1 = keras.models.Model(inputs=[users,movies,avg_ratings], outputs=r_hat)\n",
    "cf_model1.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "restore=False\n",
    "cp = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=25)\n",
    "cp1=keras.callbacks.ModelCheckpoint('./model_new_f10.hdf5', monitor='val_loss', save_best_only=True )\n",
    "cp2=keras.callbacks.CSVLogger('./model_new_f10.csv', separator=',', append=False)\n",
    "if restore:\n",
    "    cf_model1.load_weights('./model_new_f10.hdf5')\n",
    "\n",
    "model_train=cf_model1.fit([X_train[:,0],X_train[:,1],mu],\n",
    "                          Y_train,\n",
    "                          epochs=200,verbose=1,batch_size=500,\n",
    "                          validation_data=([X_val[:,0],X_val[:,1], mu_validation],Y_val),\n",
    "                         shuffle=True,callbacks=[cp,cp1,cp2], initial_epoch=0)\n",
    "cf_model1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY OF MODEL WITH F=10, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay of the F=10 model:  0.4310903332106922\n"
     ]
    }
   ],
   "source": [
    "# F=10, penalty=1e30\n",
    "# sgd=0.1\n",
    "# decay=1e19\n",
    "# model_new_f10\n",
    "Y_val1=np.squeeze(Y_val)\n",
    "Y_pred=cf_model1.predict([X_val[:,0],X_val[:,1],mu_validation])\n",
    "Y_pred1=np.squeeze(Y_pred)\n",
    "b=[]\n",
    "for i in Y_pred1:\n",
    "    b.append(round(i))\n",
    "print(\"Accurcay of the F=10 model: \", np.mean(Y_val1==b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=model_train.history\n",
    "\n",
    "plt.plot(history[\"loss\"],label=\"Loss\")\n",
    "plt.plot(history[\"val_loss\"],label=\"Validation loss\")\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL WITH F=9, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500104 samples, validate on 250052 samples\n",
      "Epoch 1/200\n",
      "500104/500104 [==============================] - 52s 105us/step - loss: 1.1128 - val_loss: 1.0006\n",
      "Epoch 2/200\n",
      "500104/500104 [==============================] - 51s 103us/step - loss: 0.9415 - val_loss: 0.9325\n",
      "Epoch 3/200\n",
      "500104/500104 [==============================] - 54s 109us/step - loss: 0.8861 - val_loss: 0.9054\n",
      "Epoch 4/200\n",
      "500104/500104 [==============================] - 51s 102us/step - loss: 0.8565 - val_loss: 0.8862\n",
      "Epoch 5/200\n",
      "500104/500104 [==============================] - 54s 108us/step - loss: 0.8375 - val_loss: 0.8766\n",
      "Epoch 6/200\n",
      "500104/500104 [==============================] - 51s 101us/step - loss: 0.8240 - val_loss: 0.8714\n",
      "Epoch 7/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.8139 - val_loss: 0.8666\n",
      "Epoch 8/200\n",
      "500104/500104 [==============================] - 51s 101us/step - loss: 0.8057 - val_loss: 0.8629\n",
      "Epoch 9/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7989 - val_loss: 0.8622\n",
      "Epoch 10/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7929 - val_loss: 0.8602\n",
      "Epoch 11/200\n",
      "500104/500104 [==============================] - 52s 105us/step - loss: 0.7876 - val_loss: 0.8595\n",
      "Epoch 12/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7828 - val_loss: 0.8599\n",
      "Epoch 13/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7783 - val_loss: 0.8587\n",
      "Epoch 14/200\n",
      "500104/500104 [==============================] - 51s 103us/step - loss: 0.7741 - val_loss: 0.8596\n",
      "Epoch 15/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7699 - val_loss: 0.8601\n",
      "Epoch 16/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7657 - val_loss: 0.8603\n",
      "Epoch 17/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7616 - val_loss: 0.8603\n",
      "Epoch 18/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7574 - val_loss: 0.8628\n",
      "Epoch 19/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7536 - val_loss: 0.8648\n",
      "Epoch 20/200\n",
      "500104/500104 [==============================] - 59s 118us/step - loss: 0.7493 - val_loss: 0.8680\n",
      "Epoch 21/200\n",
      "500104/500104 [==============================] - 54s 108us/step - loss: 0.7453 - val_loss: 0.8652\n",
      "Epoch 22/200\n",
      "500104/500104 [==============================] - 53s 107us/step - loss: 0.7410 - val_loss: 0.8643\n",
      "Epoch 23/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7367 - val_loss: 0.8644\n",
      "Epoch 24/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7321 - val_loss: 0.8636\n",
      "Epoch 25/200\n",
      "500104/500104 [==============================] - 51s 102us/step - loss: 0.7277 - val_loss: 0.8643\n",
      "Epoch 26/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.7231 - val_loss: 0.8645\n",
      "Epoch 27/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.7184 - val_loss: 0.8637\n",
      "Epoch 28/200\n",
      "500104/500104 [==============================] - 51s 101us/step - loss: 0.7138 - val_loss: 0.8635\n",
      "Epoch 29/200\n",
      "500104/500104 [==============================] - 50s 101us/step - loss: 0.7092 - val_loss: 0.8645\n",
      "Epoch 30/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.7048 - val_loss: 0.8653\n",
      "Epoch 31/200\n",
      "500104/500104 [==============================] - 54s 108us/step - loss: 0.7002 - val_loss: 0.8625\n",
      "Epoch 32/200\n",
      "500104/500104 [==============================] - 54s 108us/step - loss: 0.6959 - val_loss: 0.8682\n",
      "Epoch 33/200\n",
      "500104/500104 [==============================] - 51s 101us/step - loss: 0.6915 - val_loss: 0.8617\n",
      "Epoch 34/200\n",
      "500104/500104 [==============================] - 50s 101us/step - loss: 0.6875 - val_loss: 0.8689\n",
      "Epoch 35/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.6835 - val_loss: 0.8607\n",
      "Epoch 36/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.6793 - val_loss: 0.8649\n",
      "Epoch 37/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.6757 - val_loss: 0.8602\n",
      "Epoch 38/200\n",
      "500104/500104 [==============================] - 50s 100us/step - loss: 0.6719 - val_loss: 0.8610\n",
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "users (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movies (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mean (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 9)         4500936     users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movies_embedding (Embedding)    (None, 1, 9)         4500936     movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "user_bias (Embedding)           (None, 1, 1)         500104      users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_bias (Embedding)          (None, 1, 1)         500104      movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mean_embeddings (Embedding)     (None, 1, 1)         500104      mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dot)               (None, 1, 1)         0           user_embedding[0][0]             \n",
      "                                                                 movies_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "r_hat (Add)                     (None, 1, 1)         0           user_bias[0][0]                  \n",
      "                                                                 movie_bias[0][0]                 \n",
      "                                                                 mean_embeddings[0][0]            \n",
      "                                                                 interaction[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Flatten)                (None, 1)            0           r_hat[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 10,502,184\n",
      "Trainable params: 10,502,184\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# F=9, penalty=1e-30\n",
    "# sgd=0.1\n",
    "# decay=1e-19\n",
    "\n",
    "\n",
    "U=Train_X['UserID'].shape[0]\n",
    "M=Train_X['MovieID'].shape[0]\n",
    "mu = np.zeros(M)\n",
    "mu[:]= average_ratings   # Array of average ratings for each user.  For Training Layer\n",
    "average_ratings_shape=mu.shape[0]   #Shape of the average ratings array\n",
    "\n",
    "mu_validation = np.zeros(Validate_X['UserID'].shape[0])   # To be passed with validation data as the shape of this array should match with validation set\n",
    "mu_validation[:] = average_ratings\n",
    "users = keras.layers.Input(shape=(1,),name=\"users\")\n",
    "movies = keras.layers.Input(shape=(1,),name=\"movies\")\n",
    "avg_ratings = keras.layers.Input(shape=(1,),name=\"mean\") \n",
    "bu= keras.layers.Embedding(U,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"user_bias\"\n",
    "                              )(users)\n",
    "bm= keras.layers.Embedding(M,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"movie_bias\"\n",
    "                              )(movies)\n",
    "u= keras.layers.Embedding(average_ratings_shape,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.Constant(value=float(average_ratings)),\n",
    "                               name=\"mean_embeddings\"\n",
    "                              )(avg_ratings)\n",
    "F=9\n",
    "penalty=1e-30\n",
    "pu= keras.layers.Embedding(U,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"user_embedding\")(users)\n",
    "pm= keras.layers.Embedding(M,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"movies_embedding\")(movies)\n",
    "interaction=keras.layers.Dot(axes=-1,name=\"interaction\")([pu,pm])\n",
    "agg=keras.layers.Add(name=\"r_hat\")([bu,bm,u,interaction])\n",
    "r_hat=keras.layers.Flatten(name=\"output\")(agg)\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-19, momentum=0.9, nesterov=True)\n",
    "\n",
    "cf_model1 = keras.models.Model(inputs=[users,movies,avg_ratings], outputs=r_hat)\n",
    "cf_model1.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "restore=False\n",
    "cp = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=25)\n",
    "cp1=keras.callbacks.ModelCheckpoint('./model_new_f_9.hdf5', monitor='val_loss', save_best_only=True )\n",
    "cp2=keras.callbacks.CSVLogger('./model_new_f_9.csv', separator=',', append=False)\n",
    "if restore:\n",
    "    cf_model1.load_weights('./model_new_f_9.hdf5')\n",
    "\n",
    "model_train=cf_model1.fit([X_train[:,0],X_train[:,1],mu],\n",
    "                          Y_train,\n",
    "                          epochs=200,verbose=1,batch_size=500,\n",
    "                          validation_data=([X_val[:,0],X_val[:,1], mu_validation],Y_val),\n",
    "                         shuffle=True,callbacks=[cp,cp1,cp2], initial_epoch=0)\n",
    "cf_model1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY OF MODEL WITH F=9, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay of the F=9 model:  0.4208044726696847\n"
     ]
    }
   ],
   "source": [
    "# F=9, penalty=1e30\n",
    "# sgd=0.1\n",
    "# decau=1e19\n",
    "\n",
    "Y_val1=np.squeeze(Y_val)\n",
    "Y_pred=cf_model1.predict([X_val[:,0],X_val[:,1],mu_validation])\n",
    "Y_pred1=np.squeeze(Y_pred)\n",
    "b=[]\n",
    "for i in Y_pred1:\n",
    "    b.append(round(i))\n",
    "print(\"Accurcay of the F=9 model: \",np.mean(Y_val1==b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL WITH F=8, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500104 samples, validate on 250052 samples\n",
      "Epoch 1/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 1.1270 - val_loss: 1.0089\n",
      "Epoch 2/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.9486 - val_loss: 0.9358\n",
      "Epoch 3/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.8908 - val_loss: 0.9055\n",
      "Epoch 4/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.8599 - val_loss: 0.8870\n",
      "Epoch 5/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.8408 - val_loss: 0.8763\n",
      "Epoch 6/200\n",
      "500104/500104 [==============================] - 52s 105us/step - loss: 0.8272 - val_loss: 0.8721\n",
      "Epoch 7/200\n",
      "500104/500104 [==============================] - 55s 110us/step - loss: 0.8169 - val_loss: 0.8647\n",
      "Epoch 8/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.8090 - val_loss: 0.8651\n",
      "Epoch 9/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.8022 - val_loss: 0.8599\n",
      "Epoch 10/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7964 - val_loss: 0.8605\n",
      "Epoch 11/200\n",
      "500104/500104 [==============================] - 54s 107us/step - loss: 0.7912 - val_loss: 0.8566\n",
      "Epoch 12/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7864 - val_loss: 0.8587\n",
      "Epoch 13/200\n",
      "500104/500104 [==============================] - 55s 111us/step - loss: 0.7819 - val_loss: 0.8644\n",
      "Epoch 14/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7776 - val_loss: 0.8546\n",
      "Epoch 15/200\n",
      "500104/500104 [==============================] - 52s 103us/step - loss: 0.7732 - val_loss: 0.8553\n",
      "Epoch 16/200\n",
      "500104/500104 [==============================] - 53s 105us/step - loss: 0.7689 - val_loss: 0.8553\n",
      "Epoch 17/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7644 - val_loss: 0.8538\n",
      "Epoch 18/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7602 - val_loss: 0.8548\n",
      "Epoch 19/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7556 - val_loss: 0.8520\n",
      "Epoch 20/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7511 - val_loss: 0.8522\n",
      "Epoch 21/200\n",
      "500104/500104 [==============================] - 54s 108us/step - loss: 0.7466 - val_loss: 0.8505\n",
      "Epoch 22/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.7419 - val_loss: 0.8503\n",
      "Epoch 23/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.7373 - val_loss: 0.8494\n",
      "Epoch 24/200\n",
      "500104/500104 [==============================] - 55s 109us/step - loss: 0.7327 - val_loss: 0.8484\n",
      "Epoch 25/200\n",
      "500104/500104 [==============================] - 57s 113us/step - loss: 0.7282 - val_loss: 0.8511\n",
      "Epoch 26/200\n",
      "500104/500104 [==============================] - 56s 112us/step - loss: 0.7237 - val_loss: 0.8471\n",
      "Epoch 27/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.7193 - val_loss: 0.8529\n",
      "Epoch 28/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.7150 - val_loss: 0.8463\n",
      "Epoch 29/200\n",
      "500104/500104 [==============================] - 56s 113us/step - loss: 0.7109 - val_loss: 0.8458\n",
      "Epoch 30/200\n",
      "500104/500104 [==============================] - 57s 115us/step - loss: 0.7068 - val_loss: 0.8452\n",
      "Epoch 31/200\n",
      "500104/500104 [==============================] - 56s 112us/step - loss: 0.7029 - val_loss: 0.8506\n",
      "Epoch 32/200\n",
      "500104/500104 [==============================] - 56s 111us/step - loss: 0.6990 - val_loss: 0.8448\n",
      "Epoch 33/200\n",
      "500104/500104 [==============================] - 56s 113us/step - loss: 0.6954 - val_loss: 0.8452\n",
      "Epoch 34/200\n",
      "500104/500104 [==============================] - 60s 120us/step - loss: 0.6917 - val_loss: 0.8481\n",
      "Epoch 35/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6882 - val_loss: 0.8460\n",
      "Epoch 36/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6848 - val_loss: 0.8433\n",
      "Epoch 37/200\n",
      "500104/500104 [==============================] - 59s 118us/step - loss: 0.6814 - val_loss: 0.8433\n",
      "Epoch 38/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6783 - val_loss: 0.8434\n",
      "Epoch 39/200\n",
      "500104/500104 [==============================] - 59s 117us/step - loss: 0.6751 - val_loss: 0.8431\n",
      "Epoch 40/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6722 - val_loss: 0.8457\n",
      "Epoch 41/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6691 - val_loss: 0.8437\n",
      "Epoch 42/200\n",
      "500104/500104 [==============================] - 58s 115us/step - loss: 0.6664 - val_loss: 0.8443\n",
      "Epoch 43/200\n",
      "500104/500104 [==============================] - 57s 115us/step - loss: 0.6637 - val_loss: 0.8456\n",
      "Epoch 44/200\n",
      "500104/500104 [==============================] - 58s 115us/step - loss: 0.6611 - val_loss: 0.8429\n",
      "Epoch 45/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6584 - val_loss: 0.8437\n",
      "Epoch 46/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6560 - val_loss: 0.8433\n",
      "Epoch 47/200\n",
      "500104/500104 [==============================] - 57s 113us/step - loss: 0.6536 - val_loss: 0.8436\n",
      "Epoch 48/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6513 - val_loss: 0.8509\n",
      "Epoch 49/200\n",
      "500104/500104 [==============================] - 57s 113us/step - loss: 0.6490 - val_loss: 0.8464\n",
      "Epoch 50/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6468 - val_loss: 0.8432\n",
      "Epoch 51/200\n",
      "500104/500104 [==============================] - 59s 119us/step - loss: 0.6447 - val_loss: 0.8468\n",
      "Epoch 52/200\n",
      "500104/500104 [==============================] - 57s 115us/step - loss: 0.6426 - val_loss: 0.8431\n",
      "Epoch 53/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6406 - val_loss: 0.8431\n",
      "Epoch 54/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6387 - val_loss: 0.8476\n",
      "Epoch 55/200\n",
      "500104/500104 [==============================] - 57s 113us/step - loss: 0.6368 - val_loss: 0.8433\n",
      "Epoch 56/200\n",
      "500104/500104 [==============================] - 56s 113us/step - loss: 0.6350 - val_loss: 0.8438\n",
      "Epoch 57/200\n",
      "500104/500104 [==============================] - 57s 114us/step - loss: 0.6332 - val_loss: 0.8443\n",
      "Epoch 58/200\n",
      "500104/500104 [==============================] - 58s 115us/step - loss: 0.6315 - val_loss: 0.8469\n",
      "Epoch 59/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6298 - val_loss: 0.8438\n",
      "Epoch 60/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6282 - val_loss: 0.8457\n",
      "Epoch 61/200\n",
      "500104/500104 [==============================] - 58s 116us/step - loss: 0.6267 - val_loss: 0.8443\n",
      "Epoch 62/200\n",
      "500104/500104 [==============================] - 60s 120us/step - loss: 0.6251 - val_loss: 0.8464\n",
      "Epoch 63/200\n",
      "500104/500104 [==============================] - 57s 113us/step - loss: 0.6237 - val_loss: 0.8488\n",
      "Epoch 64/200\n",
      "500104/500104 [==============================] - 53s 107us/step - loss: 0.6222 - val_loss: 0.8458\n",
      "Epoch 65/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.6208 - val_loss: 0.8453\n",
      "Epoch 66/200\n",
      "500104/500104 [==============================] - 52s 104us/step - loss: 0.6195 - val_loss: 0.8447\n",
      "Epoch 67/200\n",
      "500104/500104 [==============================] - 53s 106us/step - loss: 0.6181 - val_loss: 0.8448\n",
      "Epoch 68/200\n",
      "500104/500104 [==============================] - 55s 109us/step - loss: 0.6169 - val_loss: 0.8512\n",
      "Epoch 69/200\n",
      "500104/500104 [==============================] - 52s 105us/step - loss: 0.6156 - val_loss: 0.8453\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "users (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movies (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mean (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 8)         4000832     users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movies_embedding (Embedding)    (None, 1, 8)         4000832     movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "user_bias (Embedding)           (None, 1, 1)         500104      users[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_bias (Embedding)          (None, 1, 1)         500104      movies[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mean_embeddings (Embedding)     (None, 1, 1)         500104      mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dot)               (None, 1, 1)         0           user_embedding[0][0]             \n",
      "                                                                 movies_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "r_hat (Add)                     (None, 1, 1)         0           user_bias[0][0]                  \n",
      "                                                                 movie_bias[0][0]                 \n",
      "                                                                 mean_embeddings[0][0]            \n",
      "                                                                 interaction[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "output (Flatten)                (None, 1)            0           r_hat[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,501,976\n",
      "Trainable params: 9,501,976\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# F=8, penalty=1e35\n",
    "# sgd=0.1\n",
    "# decau=1e19\n",
    "# model_new_f10\n",
    "\n",
    "\n",
    "U=Train_X['UserID'].shape[0]\n",
    "M=Train_X['MovieID'].shape[0]\n",
    "mu = np.zeros(M)\n",
    "mu[:]= average_ratings   # Array of average ratings for each user.  For Training Layer\n",
    "average_ratings_shape=mu.shape[0]   #Shape of the average ratings array\n",
    "\n",
    "mu_validation = np.zeros(Validate_X['UserID'].shape[0])   # To be passed with validation data as the shape of this array should match with validation set\n",
    "mu_validation[:] = average_ratings\n",
    "users = keras.layers.Input(shape=(1,),name=\"users\")\n",
    "movies = keras.layers.Input(shape=(1,),name=\"movies\")\n",
    "avg_ratings = keras.layers.Input(shape=(1,),name=\"mean\") \n",
    "bu= keras.layers.Embedding(U,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"user_bias\"\n",
    "                              )(users)\n",
    "bm= keras.layers.Embedding(M,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.RandomNormal(stddev=0.0001),\n",
    "                               name=\"movie_bias\"\n",
    "                              )(movies)\n",
    "u= keras.layers.Embedding(average_ratings_shape,1,input_length=1,\n",
    "                              embeddings_initializer=keras.initializers.Constant(value=float(average_ratings)),\n",
    "                               name=\"mean_embeddings\"\n",
    "                              )(avg_ratings)\n",
    "F=8\n",
    "penalty=1e-35\n",
    "pu= keras.layers.Embedding(U,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"user_embedding\")(users)\n",
    "pm= keras.layers.Embedding(M,F,input_length=1,\n",
    "                               embeddings_initializer=keras.initializers.RandomNormal(stddev=1/np.sqrt(F)),\n",
    "                               embeddings_regularizer=keras.regularizers.l2(penalty),\n",
    "                               name=\"movies_embedding\")(movies)\n",
    "interaction=keras.layers.Dot(axes=-1,name=\"interaction\")([pu,pm])\n",
    "agg=keras.layers.Add(name=\"r_hat\")([bu,bm,u,interaction])\n",
    "r_hat=keras.layers.Flatten(name=\"output\")(agg)\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-19, momentum=0.9, nesterov=True)\n",
    "\n",
    "cf_model1 = keras.models.Model(inputs=[users,movies,avg_ratings], outputs=r_hat)\n",
    "cf_model1.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "restore=False\n",
    "cp = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=25)\n",
    "cp1=keras.callbacks.ModelCheckpoint('./model_new_f_8.hdf5', monitor='val_loss', save_best_only=True )\n",
    "cp2=keras.callbacks.CSVLogger('./model_new_f_8.csv', separator=',', append=False)\n",
    "if restore:\n",
    "    cf_model1.load_weights('./model_new_f_8.hdf5')\n",
    "\n",
    "model_train=cf_model1.fit([X_train[:,0],X_train[:,1],mu],\n",
    "                          Y_train,\n",
    "                          epochs=200,verbose=1,batch_size=500,\n",
    "                          validation_data=([X_val[:,0],X_val[:,1], mu_validation],Y_val),\n",
    "                         shuffle=True,callbacks=[cp,cp1,cp2], initial_epoch=0)\n",
    "cf_model1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY OF MODEL WITH F=8, Penalty = 1e-30, Decay Rate = 1e-19, Batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay of the F=8 model:  0.43220210196279174\n"
     ]
    }
   ],
   "source": [
    "# F=8, penalty=1e-35\n",
    "# sgd=0.1\n",
    "# decay=1e19\n",
    "\n",
    "Y_val1=np.squeeze(Y_val)\n",
    "Y_pred3=cf_model1.predict([X_val[:,0],X_val[:,1],mu_validation])\n",
    "Y_pred31=np.squeeze(Y_pred3)\n",
    "b=[]\n",
    "for i in Y_pred31:\n",
    "    b.append(round(i))\n",
    "print(\"Accurcay of the F=8 model: \",np.mean(Y_val1==b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history=model_train.history\n",
    "\n",
    "plt.plot(history[\"loss\"],label=\"Loss\")\n",
    "plt.plot(history[\"val_loss\"],label=\"Validation loss\")\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING ACCURACY OF POPULARITY VS MODELS WITH F=8,9,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay our candidate model:  0.4334920996748689\n"
     ]
    }
   ],
   "source": [
    "Y_test1=np.squeeze(Y_test)\n",
    "Y_pred_test=cf_model1.predict([X_test[:,0],X_test[:,1],mu_test])\n",
    "Y_pred_test1=np.squeeze(Y_pred_test)\n",
    "b=[]\n",
    "for i in Y_pred_test1:\n",
    "    b.append(round(i))\n",
    "print(\"Accurcay our candidate model: \",np.mean(Y_test1==b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "cf_model12=load_model('./model_new_1.hdf5')\n",
    "cf_model12.load_weights('./model_new_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay of the popularity model:  0.42243444389789364\n"
     ]
    }
   ],
   "source": [
    "Y_test1=np.squeeze(Y_test)\n",
    "Y_pred_test2=cf_model12.predict([X_test[:,0],X_test[:,1],mu_test])\n",
    "Y_pred_test2=np.squeeze(Y_pred_test2)\n",
    "b=[]\n",
    "for i in Y_pred_test2:\n",
    "    b.append(round(i))\n",
    "print(\"Accurcay of the popularity model: \",np.mean(Y_test1==b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARY:\n",
    "\n",
    "According to the project requirements, a popularity model was trained with F=0 which gave an accuracy of about 42.56%\n",
    "\n",
    "Models with F=8,9,10 were trained and had accuracy of 43.43%, which is slightly better than the popularity model.  \n",
    "\n",
    "#### FUTURE SCOPE:\n",
    "\n",
    "Models can be trained with different learning rates like 0.01, 0.001, 0.05 and penalty may be reduced to  1e-50 for better results. \n",
    "\n",
    "Batch size can be tuned to train the model quicker, however it may have very little effect on how well our models get  trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
